# LOOP Impact Model Configuration
# Version: 1.2.0
# Last Updated: 2025-12-27
#
# Changelog (v1.2.0):
# - evaluation_windows 섹션 추가 (B 점수 평가 기간 정의)
# - realized_impact에 window_id, time_range, metrics_snapshot 추가
# - status_mapping 추가 (realized_status → verdict/outcome)
# - metrics_snapshot 제약 정의
#
# Changelog (v1.1.0):
# - realized_status에 학습 기반 상태 추가 (succeeded, failed_but_high_signal, etc.)
# - learning_value 정의 추가 (정보 엔트로피 감소량)
# - Evidence 확장 필드 정의 추가 (impact_metric, learning_value)
#
# 이 파일은 Impact A/B 점수 계산에 사용되는 모든 상수를 정의합니다.
# build_impact.py, API autofill 등에서 참조합니다.

version: "1.3.0"  # v5.3: evidence_quality_meta 추가

# ============================================
# Tier 정의
# ============================================
tiers:
  strategic:
    description: "비전/전략에 직접 기여하는 핵심 프로젝트"
    examples:
      - "신규 시장 진출"
      - "핵심 제품 출시"
      - "전략적 파트너십"

  enabling:
    description: "전략 실행을 가속하는 기반 프로젝트"
    examples:
      - "인프라 구축"
      - "팀 채용"
      - "프로세스 개선"

  operational:
    description: "일상 운영을 유지하는 필수 프로젝트"
    examples:
      - "유지보수"
      - "정기 업데이트"
      - "운영 지원"

# ============================================
# Expected Score (A) 계산
# ============================================
# ExpectedScore = magnitude_points[tier][magnitude] × confidence

magnitude_points:
  strategic:
    high: 10
    mid: 6
    low: 3
  enabling:
    high: 5
    mid: 3
    low: 1.5
  operational:
    high: 2
    mid: 1
    low: 0.5

# impact_magnitude 정의
magnitude_levels:
  high:
    description: "핵심 지표에 직접적이고 큰 영향"
    threshold: "목표 달성의 30% 이상 기여"
  mid:
    description: "지표에 중간 수준의 영향"
    threshold: "목표 달성의 10-30% 기여"
  low:
    description: "지표에 간접적이거나 작은 영향"
    threshold: "목표 달성의 10% 미만 기여"

# confidence 범위
confidence:
  min: 0.0
  max: 1.0
  default: 0.7
  guidelines:
    high: 0.8    # 실행 계획이 명확하고 리스크가 낮음
    medium: 0.5  # 불확실성이 있지만 관리 가능
    low: 0.3     # 리스크가 높거나 실험적

# ============================================
# Realized Score (B) 계산
# ============================================
# RealizedScore = normalized_delta × strength_mult × attribution_share

strength_multipliers:
  strong: 1.0   # 정량적 데이터, 명확한 인과관계
  medium: 0.7   # 정성적 증거, 합리적 추론
  weak: 0.4     # 간접 증거, 약한 연관성

# normalized_delta 범위
normalized_delta:
  min: 0.0
  max: 1.0
  description: "목표 대비 실제 달성 비율 (0=미달, 1=완전달성)"

# attribution_share 범위
attribution_share:
  min: 0.0
  max: 1.0
  default: 1.0
  description: "해당 프로젝트의 기여 비율 (다른 요인 배제)"

# ============================================
# Realized Status 정의
# ============================================
realized_status:
  planned:
    description: "아직 시작 전"
    can_have_evidence: false
    outcome_type: null

  in_progress:
    description: "진행 중, 부분 증거 가능"
    can_have_evidence: true
    outcome_type: null

  completed:
    description: "완료, 최종 증거 필요"
    can_have_evidence: true
    outcome_type: neutral

  validated:
    description: "증거 검증 완료"
    can_have_evidence: true
    outcome_type: neutral

  # === 학습 기반 상태 (v1.1 추가) ===
  succeeded:
    description: "목표 달성 + 가설 검증됨"
    can_have_evidence: true
    outcome_type: success

  failed_but_high_signal:
    description: "목표 미달 + 명확한 학습 (전략 변경 근거)"
    can_have_evidence: true
    outcome_type: learning_success
    # 이 상태는 RealizedScore가 낮아도 "전략적 가치"가 있음을 표시

  failed_low_signal:
    description: "목표 미달 + 노이즈가 많아 판단 어려움"
    can_have_evidence: true
    outcome_type: failure

  inconclusive:
    description: "외부 요인으로 결과 판단 불가"
    can_have_evidence: true
    outcome_type: null

# ============================================
# Learning Value 정의 (v1.1 추가)
# ============================================
# 실패해도 배운 게 있는가? (정보 엔트로피 감소량)
learning_value:
  high:
    description: "다음 전략 선택을 명확히 바꿈"
    criteria:
      - "주요 가설이 명확히 반증됨"
      - "전략 옵션이 줄어듦"
      - "의사결정이 쉬워짐"
    examples:
      - "와디즈 메시지 전략 폐기 확정"
      - "사회적 증거 없이 전환 불가 확인"

  medium:
    description: "가설 일부 검증/반증"
    criteria:
      - "일부 가설만 검증됨"
      - "추가 실험 필요"
    examples:
      - "가격 문제인지 메시지 문제인지 분리 안 됨"

  low:
    description: "노이즈가 많아 판단 어려움"
    criteria:
      - "외부 변수 영향이 큼"
      - "데이터 품질 낮음"
      - "실험 설계 오류"
    examples:
      - "코로나 영향으로 결과 해석 불가"

# ============================================
# Roll-up 설정 (v5.1 업데이트)
# ============================================
rollup:
  # Condition 롤업: condition_contributes 필드 사용
  condition:
    method: "weighted_sum"
    formula: "sum(project.expected × project.condition_contributes[cond].weight)"
    field: "condition_contributes"

  # Track 롤업 (v5.1 신규)
  track:
    method: "weighted_sum"
    primary_formula: "sum(project.expected × 1.0)"  # parent_id = Primary Track
    secondary_formula: "sum(project.expected × project.track_contributes[trk].weight)"
    primary_field: "parent_id"        # 암묵적 weight 1.0
    secondary_field: "track_contributes"  # 명시적 weight

  # NorthStar 롤업: Condition들의 가중 합
  northstar:
    method: "weighted_sum"
    formula: "sum(condition.score × condition.weight_to_northstar)"

# ============================================
# LLM Autofill 설정
# ============================================
autofill:
  model: "claude-sonnet-4-20250514"
  max_tokens: 1024
  temperature: 0.3

  # Context 수집 제한
  context_limits:
    max_similar_projects: 3
    max_condition_chars: 2000
    max_northstar_chars: 500

# ============================================
# 검증 규칙
# ============================================
validation:
  project:
    required_fields:
      - tier
      - track
      - contributes
      - impact_magnitude
      - confidence

    contributes_rules:
      min_count: 1
      max_count: 5
      weight_sum_max: 1.0

  evidence:
    required_fields:
      - project
      - summary
      - normalized_delta
      - evidence_strength

    # 확장 필드 (권장, 필수 아님)
    extended_fields:
      - attribution_share     # 기여 비율 (기본값: 1.0)
      - impact_metric         # 측정 지표명
      - learning_value        # 학습 가치 (high/medium/low)
      - falsified_hypotheses  # 반증된 가설 목록
      - confirmed_insights    # 확인된 인사이트 목록
      - window_id             # 평가 윈도우 ID (v5.2)
      - time_range            # 평가 기간 (v5.2)

# ============================================
# Evaluation Windows (v5.2 추가)
# ============================================
# B(Realized) 점수의 평가 기간을 정의
# 점수 계산 로직은 그대로, "어떤 기간의 결과인지"를 고정

evaluation_windows:
  # 엔티티별 기본 평가 주기
  defaults:
    project:
      period: month
      format: "YYYY-MM"        # 예: 2025-12
    track:
      period: quarter
      format: "YYYY-QN"        # 예: 2025-Q4
    condition:
      period: half
      format: "YYYY-HN"        # 예: 2025-H2

  # window_id 자동 생성 규칙
  auto_fill:
    # 기준 날짜 우선순위
    base_date_priority:
      - decided              # 1순위: realized_impact.decided
      - updated              # 2순위: 문서 updated
      - today                # 3순위: 오늘 날짜 (fallback)

    # time_range 자동 계산 규칙 (하드코딩 아닌 규칙 기반)
    time_range_rules:
      YYYY-MM: "해당 월 1일..말일"
      YYYY-QN: "분기 시작일..분기 말일"  # Q1=01-01..03-31, Q2=04-01..06-30, ...
      YYYY-HN: "반기 시작일..반기 말일"  # H1=01-01..06-30, H2=07-01..12-31
      YYYY-WNN: "ISO week 기준 7일"

    # Override 우선순위
    priority:
      1: user_input          # 사용자 명시 입력
      2: auto_generated      # 기본값 자동 생성

  # 검증 규칙 (경고 수준, 에러 아님)
  warnings:
    - "window_id만 있고 time_range 없으면 → 자동 생성 + 경고"
    - "time_range만 있고 window_id 없으면 → 경고 (user 입력 유도)"
    - "window_id와 time_range 불일치 시 경고"
    - "decided 없고 updated도 없으면 → today로 생성 + 경고"

# ============================================
# realized_status → verdict/outcome 매핑
# ============================================
# retro→evidence의 realized_status를 Project의 verdict/outcome으로 변환

status_mapping:
  succeeded:
    verdict: go
    outcome: supported
  failed_but_high_signal:
    verdict: pivot
    outcome: inconclusive
  failed_low_signal:
    verdict: no-go
    outcome: rejected
  inconclusive:
    verdict: pending
    outcome: inconclusive

# ============================================
# metrics_snapshot 제약
# ============================================
metrics_snapshot:
  # 값 타입 제한 (중첩 불허)
  allowed_types:
    - number
    - string
    - boolean
    - "null"

  # 키 네이밍 권장
  key_format: snake_case  # 예: retention_d7, funding_pct, nps

  # 키 개수 상한
  max_keys: 20

# ============================================
# Hypothesis Linking Rules (v2 추가)
# ============================================
# Expected Impact Batch API v2에서 사용
# LLM이 가설 연결 시 준수해야 할 규칙

hypothesis_rules:
  strategic_must_validate: true  # Strategic tier는 validates 필드 권장
  max_hypotheses_per_project: 3  # 프로젝트당 최대 가설 링크 수
  weight_sum_max: 1.0             # contributes weight 합계 상한

# ============================================
# Display Formatting Rules (v2 추가)
# ============================================
# 점수 표시 및 UI 렌더링 규칙

display_rules:
  score_display_mode: "stars_5"   # "stars_5" | "normalized_10" | "raw"
  decimal_places: 2               # 소수점 자릿수
  show_breakdown: true            # 계산 과정 표시 여부
  star_thresholds:                # Score → Star 매핑
    5: [9.0, 10.0]
    4: [7.0, 9.0]
    3: [5.0, 7.0]
    2: [3.0, 5.0]
    1: [0.0, 3.0]

# ============================================
# Evidence Quality Meta (v1.3.0 추가)
# ============================================
# LOOP_PHILOSOPHY 8.1: "숫자의 신뢰 구조가 없으면 그럴듯한 점수로 끝난다"
# 이 섹션은 Evidence의 품질 메타 필드 유효값과 기본값을 정의한다.

evidence_quality_meta:
  # 데이터 출처 (어디서 생긴 증거인지)
  provenance:
    values:
      - auto      # 자동 수집 (API, 쿼리, 스크립트)
      - human     # 수기 입력 (회고, 관찰)
      - mixed     # 자동 + 수기 혼합
    default: human
    description: "Evidence가 어떻게 생성되었는지"

  # 측정 품질 (얼마나 정확한 측정인지)
  measurement_quality:
    values:
      - low       # 간접 지표, 추정치, 정성적 평가
      - medium    # 직접 지표, 일부 노이즈 있음
      - high      # 정확한 측정, 검증됨, 정량적
    default: medium
    description: "측정 방법의 신뢰도"

  # 대조군/비교 유형 (인과 추론 강도)
  counterfactual:
    values:
      - none          # 대조군 없음 (단순 관측)
      - before_after  # 전후 비교 (시계열)
      - controlled    # 통제 실험 (A/B 테스트, RCT)
    default: none
    description: "인과 추론의 강도 - controlled가 가장 강함"

  # 품질 메타가 B Score에 미치는 영향 (선택적)
  quality_adjustment:
    enabled: false  # Phase 1에서는 비활성화 (메타만 기록)
    # 향후 활성화 시:
    # - measurement_quality=high: score × 1.1
    # - counterfactual=controlled: score × 1.2
    # - provenance=auto: no adjustment (자동 수집이 항상 좋은 건 아님)
