---
entity_type: Task
entity_id: tsk-000gpt-1768236851889
entity_name: Expected Impact - Batch Fill MCP API
created: '2026-01-13'
updated: '2026-01-13'
status: todo
project_id: prj-vault-gpt
parent_id: prj-vault-gpt
assignee: 김은향
priority: medium
start_date: '2026-01-13'
due: '2026-01-13'
aliases:
- tsk-000gpt-1768236851889
tags: []
type: dev
---

# Expected Impact - Batch Fill MCP API

## Notes

### Tech Spec

**Architecture Compliance:**
- Parent Project: prj-vault-gpt (ChatGPT Vault MCP 연결)
- Architecture Pattern: FastAPI router pattern, MCP integration via fastapi-mcp
- Reuses: autofill.py (LLM suggestions), impact_calculator.py (score calculation), decision_logger.py (audit trail), pending.py (update_entity_frontmatter)

**File Structure:**
```
api/routers/impact_batch.py          # New router - 4 new endpoints
api/models/impact_batch.py           # Pydantic models (10 models)
api/main.py                          # Register router + MCP operations (modify)
```

**Implementation Details:**

**API Endpoints** (4 new + 1 verify):

1. **POST /api/mcp/impact/expected/worklist**
   - Input: `WorklistRequest` (filters: missing_expected_impact, project_status_in, limit; include: project_body, parent_chain, track_context, condition_context, candidate_hypotheses)
   - Output: `WorklistResponse` (run_id, total_matched, items array with project + context)
   - Logic: Filter projects via `VaultCache.get_all_projects()`, check `expected_impact` field presence, gather context per project via `gather_project_context()`

2. **POST /api/mcp/impact/expected/suggest-batch**
   - Input: `SuggestBatchRequest` (run_id, items array with project_id, mode, provider, constraints)
   - Output: `SuggestBatchResponse` (run_id, suggestions array with expected_impact, linking, calculated scores, warnings)
   - Logic: Loop through items, call LLM per project (NOTE: Must first refactor `/api/autofill/expected-impact` endpoint into reusable function, as `autofill_expected_impact_single` doesn't exist), calculate scores via `calculate_expected_score()`, return partial success if failures

3. **POST /api/mcp/impact/expected/preview**
   - Input: `PreviewRequest` (project_id, draft fields: tier, impact_magnitude, confidence, validates, etc.)
   - Output: `PreviewResponse` (calculated scores, diff vs current, warnings for tier/condition mismatch)
   - Logic: Load current project frontmatter, calculate scores for draft, compare with current values, validate tier/condition alignment

4. **POST /api/mcp/impact/expected/apply-batch** (CRITICAL)
   - Input: `ApplyBatchRequest` (run_id, mode=execute, commit_message, updates array with project_id + patch)
   - Output: `ApplyBatchResponse` (run_id, results per project, audit info with decision_ids, validation status)
   - Logic: Validate DERIVED_FIELDS block, loop through updates, call `update_entity_frontmatter()`, log decisions (NOTE: Requires extending `decision_logger.py` to support batch operations with `run_id`, as current `log_decision()` only accepts approve/reject and requires `review_id`), return partial success

5. **GET /api/config/impact-model** (✅ Already exists at `/api/config/impact-model` - verify MCP exposure in MCP_ALLOWED_OPERATIONS)

**Data Models** (in `models/impact_batch.py`):
```python
# Request/Response models (10 total):
- WorklistFilters(BaseModel): missing_expected_impact, project_status_in, limit
- WorklistInclude(BaseModel): project_body, parent_chain, track_context, condition_context, candidate_hypotheses
- WorklistRequest(BaseModel): filters, include
- WorklistResponse(BaseModel): run_id, total_matched, items
- SuggestBatchItem(BaseModel): project_id
- SuggestBatchRequest(BaseModel): run_id, items, mode, provider, constraints
- PreviewRequest(BaseModel): project_id, draft
- PreviewResponse(BaseModel): calculated, diff, warnings
- ApplyBatchUpdate(BaseModel): project_id, patch
- ApplyBatchRequest(BaseModel): run_id, mode, commit_message, updates
- ApplyBatchResult(BaseModel): project_id, status, updated_at, applied_fields, error
- ApplyBatchResponse(BaseModel): run_id, results, audit, validation
```

**Key Functions** (in `routers/impact_batch.py`):
```python
def get_expected_impact_worklist(filters: WorklistFilters, include_flags: WorklistInclude) -> WorklistResponse
    # Filter projects from cache, gather context for each

def gather_project_context(project: Dict, include_flags: WorklistInclude, cache) -> Dict
    # Build parent_chain (ns→mh→cond→track), fetch track/conditions, filter candidate hypotheses

async def suggest_batch(items: List[SuggestBatchItem], mode: str, provider: str, constraints: Dict) -> SuggestBatchResponse
    # Loop through items, call autofill per project, calculate scores, handle partial failures

def preview_expected_impact(project_id: str, draft: Dict) -> PreviewResponse
    # Load current project, calculate draft scores, compute diff, validate

def apply_batch(updates: List[ApplyBatchUpdate], run_id: str, commit_message: str) -> ApplyBatchResponse
    # Validate DERIVED_FIELDS block, update frontmatter atomically, log decisions, partial success
```

**Helper Functions:**
```python
def generate_run_id() -> str:
    # Format: run-YYYYMMDD-HHMMSS-{random}

def build_parent_chain(project: Dict, cache) -> List[str]:
    # Trace: parent_id → track → condition → metahypothesis → northstar

def filter_candidate_hypotheses(track_id: str, cache) -> List[Dict]:
    # Extract track_num from track_id, filter hypotheses by pattern hyp-{track_num}-*
```

**State Management:**
- Uses VaultCache (singleton) for O(1) entity lookups
- No persistent state beyond frontmatter updates
- Audit trail in `_build/decision_log.jsonl` (append-only)
- No session management (stateless API)

**Dependencies** (already installed):
- `fastapi`, `pydantic` (existing)
- `python-yaml` (for frontmatter parsing)
- Uses existing modules: `llm_service`, `impact_calculator`, `decision_logger`, `pending.py` (update_entity_frontmatter)

**Safety Guards** (CRITICAL - DO NOT SKIP):
1. **DERIVED_FIELDS block list**: `{"validated_by", "realized_sum"}` - Reject any attempt to write these fields (NOTE: `track_realized_sum` and `condition_realized_sum` don't exist in schema)
2. **Atomic file writes**: Use `update_entity_frontmatter()` from pending.py (NOTE: Current implementation does NOT use temp file + rename; consider adding true atomic write helper for production safety)
3. **Partial success pattern**: Continue processing remaining items even if one fails, return both success and error lists
4. **Audit log append-only**: Every batch operation logs to decision_log.jsonl with run_id
5. **Schema validation** (optional): Can call validate_schema.py before applying batch, but not required for MVP
6. **LLM timeout handling**: Wrap each LLM call in try/except, mark as error, continue to next project

**Edge Cases:**
1. **Empty worklist**: missing_expected_impact filter returns 0 projects → Return `{"items": [], "total_matched": 0}` (200 OK)
2. **LLM timeout on project N**: catch exception → append `{"project_id": "prj-N", "error": "LLM timeout"}` to suggestions, continue to N+1
3. **Invalid project_id in apply-batch**: `find_entity_file()` returns None → append `{"project_id": "prj-X", "status": "failed", "error": "Project not found"}`, continue
4. **DERIVED_FIELDS write attempt**: Check patch keys against block list → return 400 error immediately, do not process any updates
5. **Concurrent updates**: Last-write-wins (NAS auto-sync handles git conflicts), no optimistic locking in MVP
6. **Large batch (>50 projects)**: Enforce limit in WorklistFilters validation (ge=1, le=100), return 422 if exceeded
7. **Missing parent_id (orphan project)**: build_parent_chain returns partial chain, candidate_hypotheses returns [] (graceful degradation)
8. **Track not found**: gather_project_context sets `context["track"] = None`, does not fail the request

### Todo

**Phase 1: Models + Helper Functions**
- [ ] Create `api/models/impact_batch.py` with 12 Pydantic models (WorklistFilters, WorklistInclude, WorklistRequest, WorklistResponse, SuggestBatchItem, SuggestBatchRequest, PreviewRequest, PreviewResponse, ApplyBatchUpdate, ApplyBatchRequest, ApplyBatchResult, ApplyBatchResponse)
- [ ] Refactor `api/routers/autofill.py` to extract reusable `autofill_expected_impact_single()` function (currently logic is embedded in endpoint handler)
- [ ] Add `generate_run_id()` helper in `routers/impact_batch.py`
- [ ] Add `build_parent_chain(project, cache)` helper
- [ ] Add `filter_candidate_hypotheses(track_id, cache)` helper
- [ ] Add `gather_project_context(project, include_flags, cache)` helper

**Phase 2: Endpoint Implementation**
- [ ] Implement `POST /api/mcp/impact/expected/worklist` endpoint in `routers/impact_batch.py`
- [ ] Implement `POST /api/mcp/impact/expected/suggest-batch` endpoint (async, calls LLM per project)
- [ ] Implement `POST /api/mcp/impact/expected/preview` endpoint (score calculation + diff)
- [ ] Implement `POST /api/mcp/impact/expected/apply-batch` endpoint with DERIVED_FIELDS safety guard
- [ ] Extend `api/services/decision_logger.py` with `log_batch_apply(run_id, ...)` function to support batch audit logging

**Phase 3: Integration**
- [ ] Modify `api/main.py` to `from .routers import impact_batch` and `app.include_router(impact_batch.router)`
- [ ] Add 4 new MCP operations to `MCP_ALLOWED_OPERATIONS` list in main.py (worklist_expected_impact_*, suggest_batch_expected_impact_*, preview_expected_impact_*, apply_batch_expected_impact_*)
- [ ] Add `get_impact_model_config_api_config_impact_model_get` to MCP_ALLOWED_OPERATIONS (currently not exposed to MCP clients)

**Phase 4: Testing**
- [ ] Test worklist endpoint: `curl -X POST http://localhost:8081/api/mcp/impact/expected/worklist -H "Authorization: Bearer $LOOP_API_TOKEN" -d '{"filters": {"missing_expected_impact": true, "limit": 5}}'`
- [ ] Test suggest-batch with 2-3 projects in preview mode, verify LLM suggestions + calculated scores
- [ ] Test preview endpoint with draft tier/magnitude/confidence, verify diff generation
- [ ] Test apply-batch with 1 project, verify frontmatter update + audit log entry
- [ ] Verify NAS auto-sync commits batch changes within 15 minutes
- [ ] Test DERIVED_FIELDS block (try to write `validated_by`) → expect 400 error
- [ ] Test partial success (mix of valid + invalid project_ids) → verify results array has both success + failed entries

**Phase 5: Verification**
- [ ] Check decision_log.jsonl has entries with run_id, decision="batch_apply", applied_fields list (after implementing `log_batch_apply()`)
- [ ] Check git log shows commits from NAS auto-sync with updated project files
- [ ] Verify build passes: `cd api && poetry run pytest tests/` (if tests exist)
- [ ] Verify API docs: `http://localhost:8081/docs` shows new endpoints
- [ ] Verify MCP exposure: Check `/api/mcp/tools` includes new batch endpoints and config endpoint

## 참고

**Plan File**: `/Users/gim-eunhyang/.claude/plans/fancy-toasting-storm.md` (detailed architecture design approved by user)

**Key Design Decisions**:
- Sequential LLM calls (not batch) for safer token management
- No optimistic locking (ETag) in MVP - rely on last-write-wins
- Immediate apply (skip pending review) - user preference
- NAS auto-sync for git commit (no explicit commit in API)
- Partial success pattern (continue on failures) for robustness

**Estimated Effort**: 13-18 hours
- Worklist endpoint: 2-3 hours
- Suggest batch endpoint: 3-4 hours
- Preview endpoint: 1-2 hours
- Apply batch endpoint: 4-5 hours
- Testing: 3-4 hours
