# Implementation Plan: Impact Batch API v2

**Task**: tsk-000gpt-1768282752487
**Date**: 2026-01-13
**Codex Review**: PASSED with refinements

---

## Codex Feedback Addressed

1. **Type hints & deprecation** - Added to calculate_expected_score()
2. **Config defaults** - Added error handling for missing config keys
3. **Schema versioning** - Using impact_model_config.yml version field
4. **Backward compatibility** - New fields optional in Pydantic models
5. **Weight validation** - Using thresholds from config
6. **Error collection** - Max 100 errors per batch
7. **HTTP status codes** - 422 for validation errors, 200 with warnings

---

## Phase 0: Cross-Module Audit

**Files to verify**:
- `api/routers/impact_batch.py` - Lines 381-386, 452-457
- `api/routers/projects.py` - Lines 190-194
- `api/routers/autofill.py` - Lines 268-307
- `api/routers/ai.py` - Lines 837-883

**Action**: Confirm only impact_batch.py needs dict return.

---

## Phase 1: Bug Fix (45 min)

**File**: `api/utils/impact_calculator.py`

```python
def calculate_expected_score_with_breakdown(
    tier: str,
    magnitude: str,
    confidence: float
) -> Dict[str, Any]:
    """
    Calculate Expected Score with complete breakdown.

    Returns:
        {
            "score": float,
            "tier_points": float,
            "confidence": float,
            "tier": str,
            "magnitude": str,
            "formula": str,
            "max_score_by_tier": float,
            "normalized_10": float
        }
    """
    config = load_impact_config()
    magnitude_points = config.get("magnitude_points", {})

    if tier not in magnitude_points:
        raise ValueError(f"Invalid tier: {tier}")
    if magnitude not in magnitude_points[tier]:
        raise ValueError(f"Invalid magnitude: {magnitude}")

    confidence = max(0.0, min(1.0, confidence))
    tier_points = magnitude_points[tier][magnitude]
    score = tier_points * confidence

    # Calculate max possible for this tier
    max_score_by_tier = max(magnitude_points[tier].values())

    # Normalize to 0-10 scale
    normalized_10 = (score / max_score_by_tier) * 10 if max_score_by_tier > 0 else 0.0

    return {
        "score": score,
        "tier_points": tier_points,
        "confidence": confidence,
        "tier": tier,
        "magnitude": magnitude,
        "formula": f"{tier_points} × {confidence}",
        "max_score_by_tier": max_score_by_tier,
        "normalized_10": round(normalized_10, 2)
    }


def calculate_expected_score(
    tier: str,
    magnitude: str,
    confidence: float
) -> float:
    """
    Legacy wrapper for backward compatibility.

    DEPRECATED: Use calculate_expected_score_with_breakdown() for full details.
    This function maintained for existing callers in projects.py, autofill.py, ai.py.

    Returns:
        float: Expected score only
    """
    result = calculate_expected_score_with_breakdown(tier, magnitude, confidence)
    return result["score"]
```

---

## Phase 2: Config Files (90 min)

### File 1: `impact_model_config.yml`

```yaml
# Add after existing sections

# NEW: Hypothesis linking rules
hypothesis_rules:
  strategic_must_validate: true
  max_hypotheses_per_project: 3
  weight_sum_max: 1.0

# NEW: Display formatting rules
display_rules:
  score_display_mode: "stars_5"
  decimal_places: 2
  show_breakdown: true
  star_thresholds:
    5: [9.0, 10.0]
    4: [7.0, 9.0]
    3: [5.0, 7.0]
    2: [3.0, 5.0]
    1: [0.0, 3.0]
```

### File 2: `api/prompts/expected_impact.py`

Update `build_simple_expected_impact_prompt()` output format (line ~250):

```python
### 출력 형식 (JSON) - v5.3 Schema

{{
  "tier": {{
    "value": "strategic|enabling|operational",
    "reasoning": "판단 근거"
  }},
  "impact_magnitude": {{
    "value": "high|mid|low",
    "reasoning": "판단 근거"
  }},
  "confidence": {{
    "value": 0.0-1.0,
    "reasoning": "판단 근거"
  }},
  "summary": "1-2문장 핵심 요약",

  # NEW v5.3 fields (optional)
  "validates": ["hyp-X-XX"],
  "primary_hypothesis_id": "hyp-X-XX",
  "condition_contributes": [
    {{"condition_id": "cond-a", "weight": 0.5}}
  ],
  "track_contributes": [
    {{"track_id": "trk-N", "weight": 0.3}}
  ],
  "assumptions": ["가정 1", "가정 2"],
  "evidence_refs": ["참고 링크"],
  "linking_reason": "왜 이 가설/조건/트랙에 연결되는지"
}}

**IMPORTANT**: condition_contributes and track_contributes are TOP-LEVEL fields,
NOT nested inside expected_impact object.
```

---

## Phase 3: Pydantic Models (60 min)

**File**: `api/models/impact_batch.py`

Add at end of file:

```python
# ============================================
# v2 Response Enhancement Models
# ============================================

class OutputContract(BaseModel):
    """Required fields specification for LLM"""
    must_set: List[str] = Field(
        default=["tier", "impact_magnitude", "confidence"],
        description="Fields that must be provided"
    )
    contributes_rules: Dict[str, Any] = Field(
        default_factory=dict,
        description="Rules for condition_contributes and track_contributes"
    )
    hypothesis_rules: Dict[str, Any] = Field(
        default_factory=dict,
        description="Rules for validates and hypothesis linking"
    )


class ScoringContext(BaseModel):
    """Impact model context for LLM understanding"""
    impact_model_version: str
    tier_points: Dict[str, Dict[str, float]]
    display_rules: Dict[str, Any] = Field(default_factory=dict)


class ApplyPatch(BaseModel):
    """v5.3 compliant patch format"""
    expected_impact: Dict[str, Any] = Field(
        description="Contains: statement, metric, target (NO contributes)"
    )
    condition_contributes: Optional[List[Dict[str, Any]]] = Field(
        default=None,
        description="TOP-LEVEL: Condition contributions"
    )
    track_contributes: Optional[List[Dict[str, Any]]] = Field(
        default=None,
        description="TOP-LEVEL: Track contributions"
    )
    validates: Optional[List[str]] = Field(
        default=None,
        description="Hypothesis IDs this project validates"
    )
    primary_hypothesis_id: Optional[str] = None


class StructuredReasoning(BaseModel):
    """Audit-ready reasoning breakdown"""
    tier_reason: str = ""
    magnitude_reason: str = ""
    confidence_reason: str = ""
    assumptions: List[str] = Field(default_factory=list)
    evidence_refs: List[str] = Field(default_factory=list)
    linking_reason: str = ""


class CalculatedFields(BaseModel):
    """Complete score calculation results"""
    expected_score: float
    tier_points: float
    confidence: float
    formula: str
    max_score_by_tier: float
    normalized_10: float


class FieldValidationError(BaseModel):
    """Field-level validation error"""
    field_path: str
    error_type: str
    message: str
    expected: Optional[Any] = None
    actual: Optional[Any] = None
    constraint: Optional[str] = None


# Update existing WorklistResponse
class WorklistResponseV2(BaseModel):
    """Enhanced worklist with contract and context"""
    run_id: str
    total_matched: int
    items: List[WorklistItem]
    required_output_contract: Optional[OutputContract] = None  # NEW
    scoring_context: Optional[ScoringContext] = None  # NEW


# Update existing SuggestBatchResponse
class SuggestBatchResponseV2(BaseModel):
    """Enhanced suggestions with schema version"""
    run_id: str
    mode: str
    schema_version: str = "v2"  # NEW
    impact_model_version: Optional[str] = None  # NEW
    suggestions: List[SuggestBatchSuggestion]
    summary: Dict[str, int]


# Update SuggestBatchSuggestion
class SuggestBatchSuggestionV2(BaseModel):
    """Enhanced suggestion with apply_patch and structured reasoning"""
    project_id: str
    status: str
    suggested_fields: Dict[str, Any] = Field(
        default_factory=dict,
        description="DEPRECATED: Use apply_patch instead"
    )
    apply_patch: Optional[ApplyPatch] = None  # NEW
    calculated_fields: Optional[CalculatedFields] = None  # NEW (enhanced)
    reasoning: Optional[StructuredReasoning] = None  # NEW
    warnings: List[str] = Field(default_factory=list)
    error: Optional[str] = None
```

---

## Phase 3a: Worklist Endpoint (30 min)

**File**: `api/routers/impact_batch.py` (line ~196)

```python
@router.post("/worklist", response_model=WorklistResponseV2)
async def get_expected_impact_worklist(...):
    # ... existing code ...

    # NEW: Load config for contract + context
    try:
        config = load_impact_config()

        output_contract = OutputContract(
            must_set=["tier", "impact_magnitude", "confidence"],
            contributes_rules=config.get("validation", {}).get("project", {}).get("contributes_rules", {}),
            hypothesis_rules=config.get("hypothesis_rules", {})
        )

        scoring_context = ScoringContext(
            impact_model_version=config.get("version", "1.3.0"),
            tier_points=config.get("magnitude_points", {}),
            display_rules=config.get("display_rules", {})
        )
    except Exception as e:
        # Fallback if config loading fails
        output_contract = None
        scoring_context = None

    return WorklistResponseV2(
        run_id=run_id,
        total_matched=len(filtered),
        items=items,
        required_output_contract=output_contract,
        scoring_context=scoring_context
    )
```

---

## Phase 4: Suggest Batch Endpoint (120 min)

**File**: `api/routers/impact_batch.py` (line ~265)

Key changes:
1. Extract v5.3 fields from LLM response
2. Use `calculate_expected_score_with_breakdown()`
3. Build `ApplyPatch` with correct structure
4. Build `StructuredReasoning` and `CalculatedFields`
5. Add schema_version and impact_model_version

---

## Phase 5: Preview Endpoint (45 min)

Add weight validation and structured warnings.

---

## Phase 6: Apply Batch Endpoint (45 min)

Add field-level validation with FieldValidationError model.

---

## Testing Checklist

- [ ] Unit test: calculate_expected_score_with_breakdown() returns dict
- [ ] Unit test: calculate_expected_score() returns float
- [ ] Integration: Worklist includes contract + context
- [ ] Integration: Suggest batch builds v5.3 compliant patches
- [ ] Integration: Preview validates weights
- [ ] Integration: Apply collects field errors
- [ ] E2E: Full worklist → suggest → preview → apply flow

---

## Rollout Strategy

1. Deploy with backward compatibility (old fields still work)
2. ChatGPT MCP updates to use new fields
3. Monitor for 1 week
4. Mark old fields as deprecated in docs
5. Remove old fields in v3 (future)
